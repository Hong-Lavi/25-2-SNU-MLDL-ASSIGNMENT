{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e521099c",
   "metadata": {},
   "source": [
    "\n",
    "# Homework 1 — Linear & Logistic Regression (Clean Submission)\n",
    "\n",
    "This notebook is a **cleaned and organized** version of the working notebook, focused on the exact assignment requirements.\n",
    "It contains **Problem 1 (Linear Regression)** and **Problem 2 (Logistic Regression)**, each with parts (a)–(e).  \n",
    "For each part, we include:\n",
    "- A short **problem summary** (what is required),\n",
    "- The **minimal, correct code** to satisfy the requirement,\n",
    "- **Explanations** of *why* we do it and *what* the outputs mean.\n",
    "\n",
    "> Reproducibility: we set a fixed random seed where required and keep preprocessing consistent across parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fda32d",
   "metadata": {},
   "source": [
    "\n",
    "> **Data**  \n",
    "- `Carseats.csv` for Problem 1 (variables: `Sales`, `Price`, `Urban`, `US`).  \n",
    "- `Default.csv` for Problem 2 (variables: `default`, `student`, `balance`, `income`).\n",
    "\n",
    "> **Paths**  \n",
    "- Place the CSVs in the current working directory **or** update `DATA_DIR` accordingly.  \n",
    "- On Google Colab, either use `files.upload()` or mount Google Drive.\n",
    "\n",
    "> **Random Seed**  \n",
    "We use `RANDOM_SEED = 1` across all steps that require randomness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba5c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling & evaluation\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 1\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Data directory (adjust if needed)\n",
    "DATA_DIR = \"/Users/lavi/MLDL_25_2_ASSIGNMENT/ASSIGNMENT_1/Data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf6f7b7",
   "metadata": {},
   "source": [
    "\n",
    "> **Colab helpers (optional):**  \n",
    "Uncomment and run the cells below if you're on Google Colab and need to load files.\n",
    "\n",
    "```python\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # then read directly by filename\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = \"/content/drive/MyDrive/your_folder\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620e4df",
   "metadata": {},
   "source": [
    "# Problem 1 — Linear Regression (Carseats.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1365c",
   "metadata": {},
   "source": [
    "\n",
    "We use the **Carseats** dataset and focus on the model:\n",
    "\n",
    "\\[\n",
    "\\text{Sales} \\sim \\text{Price} + \\text{Urban} + \\text{US}\n",
    "\\]\n",
    "\n",
    "- `Urban`, `US` are qualitative predictors with two levels; we use **one dummy per predictor** (drop the baseline) to avoid the dummy-variable trap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c787612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 columns: ['Price', 'Urban_Yes', 'US_Yes']\n",
      "X1 shape: (400, 3)  y1 shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Carseats\n",
    "car_path = f\"{DATA_DIR}/Carseats.csv\"\n",
    "df1 = pd.read_csv(car_path)\n",
    "\n",
    "# Ensure correct dtypes and dummy coding\n",
    "df1['Urban'] = df1['Urban'].astype('category')\n",
    "df1['US']    = df1['US'].astype('category')\n",
    "\n",
    "# Target and predictors\n",
    "y1 = df1['Sales']\n",
    "X1 = pd.get_dummies(df1[['Price', 'Urban', 'US']], drop_first=True)  # creates Urban_Yes, US_Yes\n",
    "\n",
    "print(\"X1 columns:\", list(X1.columns))\n",
    "print(\"X1 shape:\", X1.shape, \" y1 shape:\", y1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7af1ff9",
   "metadata": {},
   "source": [
    "\n",
    "## (a) Fit Linear Regression with scikit-learn and report coefficients and R²\n",
    "\n",
    "**What to do:** Fit OLS using scikit-learn (`LinearRegression`) on the full data and report the intercept, coefficients, and R².\n",
    "\n",
    "**Why:** This provides a baseline OLS fit using a standard library implementation.\n",
    "\n",
    "**Output meaning:**\n",
    "- Intercept \\(\\beta_0\\): expected Sales at baseline (Urban=No, US=No) & Price=0 (extrapolation).\n",
    "- Coefficients: slope effects of each predictor on Sales.\n",
    "- \\(R^2\\): fraction of variance in Sales explained by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb64280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 13.043468936764892\n",
      "Coefficients: {'Price': np.float64(-0.05445884917758218), 'Urban_Yes': np.float64(-0.021916150814141), 'US_Yes': np.float64(1.200572697794117)}\n",
      "R^2 (full data): 0.23927539218405547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lin1 = LinearRegression(fit_intercept=True)\n",
    "lin1.fit(X1, y1)\n",
    "\n",
    "r2_full = lin1.score(X1, y1)\n",
    "print(\"Intercept:\", lin1.intercept_)\n",
    "print(\"Coefficients:\", dict(zip(X1.columns, lin1.coef_)))\n",
    "print(\"R^2 (full data):\", r2_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78426f",
   "metadata": {},
   "source": [
    "\n",
    "## (b) Write the model and interpret coefficients\n",
    "\n",
    "Model:\n",
    "\\[\n",
    "\\widehat{\\text{Sales}} = \\beta_0 + \\beta_1 \\cdot \\text{Price} + \\beta_2 \\cdot \\text{Urban\\_Yes} + \\beta_3 \\cdot \\text{US\\_Yes}\n",
    "\\]\n",
    "\n",
    "- **Price**: expected change in Sales per unit increase in Price.\n",
    "- **Urban\\_Yes**: difference in baseline Sales for Urban=Yes vs Urban=No.\n",
    "- **US\\_Yes**: difference in baseline Sales for US=Yes vs US=No.\n",
    "\n",
    "We interpret signs and magnitudes based on the fitted coefficients above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6ea67",
   "metadata": {},
   "source": [
    "\n",
    "## (c) Closed-form OLS: \\(\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y\\)\n",
    "\n",
    "**What to do:** Construct a design matrix \\(X\\) with an intercept column and one dummy per qualitative predictor (as above), then compute \\(\\hat{\\beta}\\) via the closed-form expression. Compare with scikit-learn results.\n",
    "\n",
    "**Note:** Use the **same coding choices** as in part (a).\n",
    "\n",
    "**Output meaning:** \\(\\hat{\\beta}\\) is a \\((p{+}1) \\times 1\\) vector (intercept + p coefficients).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce5bcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Estimate (closed-form)\n",
      "Intercept               13.043469\n",
      "Price                   -0.054459\n",
      "Urban_Yes               -0.021916\n",
      "US_Yes                   1.200573\n",
      "\n",
      "Comparison closed-form vs sklearn:\n",
      "           ClosedForm    sklearn      abs_diff\n",
      "Intercept   13.043469  13.043469  1.421085e-14\n",
      "Price       -0.054459  -0.054459  1.595946e-16\n",
      "Urban_Yes   -0.021916  -0.021916  4.836409e-15\n",
      "US_Yes       1.200573   1.200573  6.661338e-16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build design matrix with intercept\n",
    "X1_closed = np.column_stack([np.ones((X1.shape[0], 1)), X1.values]).astype(float)  # (n, p+1)\n",
    "y1_mat = y1.values.reshape(-1, 1)                                    # (n, 1)\n",
    "\n",
    "# Closed-form estimator\n",
    "beta_hat_1 = np.linalg.inv(X1_closed.T @ X1_closed) @ (X1_closed.T @ y1_mat)  # (p+1, 1)\n",
    "\n",
    "colnames1 = ['Intercept'] + list(X1.columns)\n",
    "beta_df1 = pd.DataFrame(beta_hat_1.flatten(), index=colnames1, columns=['Estimate (closed-form)'])\n",
    "print(beta_df1)\n",
    "\n",
    "# Compare with sklearn\n",
    "coef_sklearn = np.r_[lin1.intercept_, lin1.coef_]\n",
    "cmp1 = pd.DataFrame({'ClosedForm': beta_hat_1.flatten(), 'sklearn': coef_sklearn}, index=colnames1)\n",
    "cmp1['abs_diff'] = np.abs(cmp1['ClosedForm'] - cmp1['sklearn'])\n",
    "print(\"\\nComparison closed-form vs sklearn:\")\n",
    "print(cmp1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e998d",
   "metadata": {},
   "source": [
    "\n",
    "## (d) t-statistics, standard errors, and p-values (α=0.05)\n",
    "\n",
    "**What to do:** Compute the residual variance \\(\\hat{\\sigma}^2\\), the covariance matrix \\(\\hat{\\sigma}^2 (X^\\top X)^{-1}\\), then standard errors, t-stats, and two-sided p-values.\n",
    "\n",
    "**Degrees of freedom:** \\(df = n - (p+1)\\).\n",
    "\n",
    "**Output meaning:** significance of each coefficient at α=0.05.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a53b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Estimate  Std.Error    t value  Pr(>|t|)\n",
      "Intercept  13.043469   0.651012  20.035674  0.000000\n",
      "Price      -0.054459   0.005242 -10.389232  0.000000\n",
      "Urban_Yes  -0.021916   0.271650  -0.080678  0.935739\n",
      "US_Yes      1.200573   0.259042   4.634673  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/sb20bp956_bgbntjhp461wbm0000gn/T/ipykernel_78763/4059263183.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  sigma_sq_1 = float((eps1.T @ eps1) / df1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predictions & residuals\n",
    "y1_hat = X1_closed @ beta_hat_1\n",
    "eps1 = y1_mat - y1_hat\n",
    "\n",
    "n1, p1_plus_1 = X1_closed.shape  # p1_plus_1 = p+1\n",
    "df1 = n1 - p1_plus_1\n",
    "\n",
    "sigma_sq_1 = float((eps1.T @ eps1) / df1)\n",
    "cov_beta_1 = sigma_sq_1 * np.linalg.inv(X1_closed.T @ X1_closed)\n",
    "se_1 = np.sqrt(np.diag(cov_beta_1))\n",
    "\n",
    "t_stat_1 = beta_hat_1.flatten() / se_1\n",
    "p_vals_1 = 2 * (1 - stats.t.cdf(np.abs(t_stat_1), df=df1))\n",
    "\n",
    "t_table_1 = pd.DataFrame({\n",
    "    'Estimate': beta_hat_1.flatten(),\n",
    "    'Std.Error': se_1,\n",
    "    't value': t_stat_1,\n",
    "    'Pr(>|t|)': p_vals_1\n",
    "}, index=colnames1)\n",
    "\n",
    "print(t_table_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d560b96",
   "metadata": {},
   "source": [
    "\n",
    "## (e) 70/30 Train–Validation and 5-fold Cross-Validation\n",
    "\n",
    "**What to do:**  \n",
    "1. Random 70/30 split (report the seed).  \n",
    "2. Fit on the training set; report **validation-set \\(R^2\\)** and **MSE**.  \n",
    "3. Perform **5-fold CV** and report mean \\(R^2\\) and MSE.  \n",
    "4. Briefly discuss differences.\n",
    "\n",
    "**Why:** Assesses **generalization** instead of in-sample fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d860b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation R^2: 0.2849\n",
      "Validation MSE: 5.6138\n",
      "Cross-validated R^2 (mean): 0.2038\n",
      "Cross-validated MSE (mean): 6.1903\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 70/30 split\n",
    "X1_tr, X1_va, y1_tr, y1_va = train_test_split(X1, y1, test_size=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "lm1 = LinearRegression(fit_intercept=True)\n",
    "lm1.fit(X1_tr, y1_tr)\n",
    "\n",
    "y1_pred_va = lm1.predict(X1_va)\n",
    "r2_val_1 = r2_score(y1_va, y1_pred_va)\n",
    "mse_val_1 = mean_squared_error(y1_va, y1_pred_va)\n",
    "\n",
    "print(f\"Validation R^2: {r2_val_1:.4f}\")\n",
    "print(f\"Validation MSE: {mse_val_1:.4f}\")\n",
    "\n",
    "# 5-fold Cross-Validation (mean R^2 and MSE)\n",
    "r2_cv_1 = cross_val_score(lm1, X1, y1, cv=5, scoring='r2').mean()\n",
    "mse_cv_1 = -cross_val_score(lm1, X1, y1, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "print(f\"Cross-validated R^2 (mean): {r2_cv_1:.4f}\")\n",
    "print(f\"Cross-validated MSE (mean): {mse_cv_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d8fd5",
   "metadata": {},
   "source": [
    "# Problem 2 — Logistic Regression (Default.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca1a2f0",
   "metadata": {},
   "source": [
    "\n",
    "We model **default** (Yes/No) as a function of **income** and **balance** (and later `student`).  \n",
    "We encode `default` as 1(Yes)/0(No), and **standardize** features for stable optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0bd38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2 shape: (10000, 2)  y2 shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Default\n",
    "def_path = f\"{DATA_DIR}/Default.csv\"\n",
    "df2 = pd.read_csv(def_path)\n",
    "\n",
    "# Target encoding\n",
    "df2['default_bin'] = df2['default'].map({'No': 0, 'Yes': 1}).astype(int)\n",
    "\n",
    "X2 = df2[['income', 'balance']]\n",
    "y2 = df2['default_bin']\n",
    "\n",
    "print(\"X2 shape:\", X2.shape, \" y2 shape:\", y2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bb5cc",
   "metadata": {},
   "source": [
    "\n",
    "## (a) Logistic Regression via scikit-learn (MLE) + Training Log-Likelihood\n",
    "\n",
    "- Pipeline: `StandardScaler` + `LogisticRegression(penalty='none', solver='lbfgs')`  \n",
    "- Report **intercept, coefficients**, and **training log-likelihood** (sum, not mean).\n",
    "\n",
    "**Why penalty='none'?** To match pure MLE without regularization.  \n",
    "**Why standardize?** To improve numerical conditioning (features on comparable scales).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "784f9134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β (std-scale) = [-6.1093768   0.27915846  2.72232674]\n",
      "Training log-likelihood: -789.4877437837663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe2a = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=RANDOM_SEED))\n",
    "])\n",
    "pipe2a.fit(X2, y2)\n",
    "\n",
    "clf2a = pipe2a.named_steps['clf']\n",
    "beta2a = np.r_[clf2a.intercept_[0], clf2a.coef_[0]]\n",
    "print(\"β (std-scale) =\", beta2a)\n",
    "\n",
    "p_train2 = pipe2a.predict_proba(X2)[:, 1]\n",
    "n2 = len(y2)\n",
    "train_loglik2 = -log_loss(y2, p_train2, normalize=False)  # sum log-likelihood\n",
    "print(\"Training log-likelihood:\", train_loglik2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522f270",
   "metadata": {},
   "source": [
    "\n",
    "## (b) Model Equation & Interpretation (Signs and Odds Ratios)\n",
    "\n",
    "Model:\n",
    "\\[\n",
    "\\log\\frac{p_i}{1-p_i} = \\beta_0 + \\beta_1 \\cdot income_i + \\beta_2 \\cdot balance_i\n",
    "\\]\n",
    "\n",
    "- Sign of \\(\\beta_1\\): effect of income on default odds.  \n",
    "- Sign of \\(\\beta_2\\): effect of balance on default odds.  \n",
    "- **Odds ratio**: \\(e^{\\beta_j}\\). With standardized predictors, “1 unit” = “1 standard deviation”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a8e5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income OR = 1.322\n",
      "Balance OR = 15.216\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coef_income, coef_balance = clf2a.coef_[0]\n",
    "or_income = np.exp(coef_income)\n",
    "or_balance = np.exp(coef_balance)\n",
    "\n",
    "print(f\"Income OR = {or_income:.3f}\")\n",
    "print(f\"Balance OR = {or_balance:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eec7fd",
   "metadata": {},
   "source": [
    "\n",
    "## (c) Manual MLE by Direct Log-Likelihood Maximization\n",
    "\n",
    "We construct \\(X=[1, income_{std}, balance_{std}]\\), define \\(-\\log L(\\beta)\\) and its gradient,\n",
    "and use BFGS to minimize it. We then compare the coefficients and log-likelihood with part (a).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58c6c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β (manual, std-scale) = [-6.12556642  0.27750793  2.73145174]\n",
      "Max log-likelihood (manual): -789.4831350811944\n",
      "          param  sklearn(2a)  manual(2c)  abs_diff\n",
      "0     Intercept    -6.109377   -6.125566  0.016190\n",
      "1   income(std)     0.279158    0.277508  0.001651\n",
      "2  balance(std)     2.722327    2.731452  0.009125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use the same scaler as in (a)\n",
    "scaler2 = pipe2a.named_steps['scaler']\n",
    "X2_std = scaler2.transform(X2)\n",
    "X2_mat = np.column_stack([np.ones(len(X2_std)), X2_std]).astype(float)\n",
    "y2_vec = y2.values.astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(z, -35, 35)))\n",
    "\n",
    "def neg_loglik2(beta):\n",
    "    z = X2_mat @ beta\n",
    "    p = sigmoid(z)\n",
    "    eps = 1e-12\n",
    "    return -np.sum(y2_vec*np.log(p + eps) + (1 - y2_vec)*np.log(1 - p + eps))\n",
    "\n",
    "def grad_neg_loglik2(beta):\n",
    "    z = X2_mat @ beta\n",
    "    p = sigmoid(z)\n",
    "    return X2_mat.T @ (p - y2_vec)\n",
    "\n",
    "beta0 = np.zeros(X2_mat.shape[1])\n",
    "res2 = minimize(neg_loglik2, beta0, jac=grad_neg_loglik2, method='BFGS', options={'gtol':1e-8, 'maxiter':1000})\n",
    "beta2c = res2.x\n",
    "loglik2c = -res2.fun\n",
    "\n",
    "print(\"β (manual, std-scale) =\", beta2c)\n",
    "print(\"Max log-likelihood (manual):\", loglik2c)\n",
    "\n",
    "# Compare with (a)\n",
    "cmp2 = pd.DataFrame({\n",
    "    'param': ['Intercept','income(std)','balance(std)'],\n",
    "    'sklearn(2a)': beta2a,\n",
    "    'manual(2c)': beta2c,\n",
    "})\n",
    "cmp2['abs_diff'] = np.abs(cmp2['sklearn(2a)'] - cmp2['manual(2c)'])\n",
    "print(cmp2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e3270",
   "metadata": {},
   "source": [
    "\n",
    "## (d) 70/30 Validation (Misclassification Rate) and 5-Fold CV\n",
    "\n",
    "- **Split** the data into 70%/30% (seed reported).\n",
    "- **Fit** the (2a) pipeline on the training set.\n",
    "- **Validation** misclassification rate using threshold 0.5.\n",
    "- **5-fold CV** mean misclassification rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47e0be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9760\n",
      "Validation Misclassification Rate: 0.0240\n",
      "5-Fold Mean Accuracy: 0.9735\n",
      "5-Fold Mean Misclassification Rate: 0.0265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X2_tr, X2_va, y2_tr, y2_va = train_test_split(X2, y2, test_size=0.3, random_state=RANDOM_SEED, stratify=y2)\n",
    "\n",
    "pipe2d = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=RANDOM_SEED))\n",
    "])\n",
    "pipe2d.fit(X2_tr, y2_tr)\n",
    "\n",
    "proba_va = pipe2d.predict_proba(X2_va)[:, 1]\n",
    "yhat_va = (proba_va >= 0.5).astype(int)\n",
    "\n",
    "val_acc = accuracy_score(y2_va, yhat_va)\n",
    "val_miscl = 1 - val_acc\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Misclassification Rate: {val_miscl:.4f}\")\n",
    "\n",
    "cv_acc = cross_val_score(pipe2d, X2, y2, cv=5, scoring='accuracy')\n",
    "cv_miscl = 1 - cv_acc.mean()\n",
    "\n",
    "print(f\"5-Fold Mean Accuracy: {cv_acc.mean():.4f}\")\n",
    "print(f\"5-Fold Mean Misclassification Rate: {cv_miscl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81f3ff",
   "metadata": {},
   "source": [
    "\n",
    "## (e) Add `student` Dummy and Compare 5-Fold CV Error\n",
    "\n",
    "We add a dummy \\(1\\{student=\\text{Yes}\\}\\) and evaluate the mean misclassification rate via 5-fold CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3e8e87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2e) 5-Fold Mean Accuracy: 0.9732\n",
      "(2e) 5-Fold Mean Misclassification Rate: 0.0268\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2['student_bin'] = df2['student'].map({'No': 0, 'Yes': 1}).astype(int)\n",
    "X2e = df2[['income','balance','student_bin']]\n",
    "\n",
    "pipe2e = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=RANDOM_SEED))\n",
    "])\n",
    "\n",
    "cv_acc_2e = cross_val_score(pipe2e, X2e, y2, cv=5, scoring='accuracy')\n",
    "cv_miscl_2e = 1 - cv_acc_2e.mean()\n",
    "\n",
    "print(f\"(2e) 5-Fold Mean Accuracy: {cv_acc_2e.mean():.4f}\")\n",
    "print(f\"(2e) 5-Fold Mean Misclassification Rate: {cv_miscl_2e:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7746a08",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- All random splits use `random_state=1` for reproducibility.\n",
    "- Standardization is kept consistent wherever required.\n",
    "- Cross-validated metrics are averaged across folds.\n",
    "- Closed-form OLS and sklearn results (Problem 1) match up to numeric precision.\n",
    "- Manual MLE (Problem 2) matches sklearn’s MLE (penalty='none') up to optimizer tolerance.\n",
    "\n",
    "**End of cleaned notebook.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
